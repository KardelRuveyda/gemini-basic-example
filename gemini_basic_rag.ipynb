{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52602e2",
   "metadata": {},
   "source": [
    "## PDF Y√ºklemesinin Ger√ßekle≈ütirlimesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1d8ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith Tracing: true\n",
      "‚úÖ LangSmith Project: gemini-rag-project\n",
      "‚úÖ LangSmith API Key: Configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# LangSmith tracing'i aktif et\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"gemini-rag-project\"\n",
    "\n",
    "# Ayarlarƒ± doƒürula\n",
    "print(\"‚úÖ LangSmith Tracing:\", os.getenv(\"LANGCHAIN_TRACING_V2\"))\n",
    "print(\"‚úÖ LangSmith Project:\", os.getenv(\"LANGCHAIN_PROJECT\"))\n",
    "print(\"‚úÖ LangSmith API Key:\", \"Configured\" if os.getenv(\"LANGCHAIN_API_KEY\") else \"‚ùå Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839d325",
   "metadata": {},
   "source": [
    "## Manuel Zaman √ñl√ß√ºm√º ƒ∞√ßin Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "329c6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_time(operation_name):\n",
    "    \"\"\"Decorator to measure execution time\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f\"‚è±Ô∏è {operation_name}: {duration:.2f} seconds\")\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef1ddc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è PDF Loading: 1.10 seconds\n",
      "üìÑ Loaded 15 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"load_pdf\", run_type=\"chain\")\n",
    "@measure_time(\"PDF Loading\")\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "file_path = \"attentionisallyouneedgemini.pdf\"\n",
    "data = load_pdf(file_path)\n",
    "print(f\"üìÑ Loaded {len(data)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c057c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruveyda.cetin\\AppData\\Local\\miniconda3\\envs\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"attentionisallyouneedgemini.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "969a0585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a01597",
   "metadata": {},
   "source": [
    "## Veriyi par√ßalara ayƒ±rma(Chunking i≈ülemi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a377a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cb94938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Document Chunking: 0.00 seconds\n",
      "üìù Created 48 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"chunk_documents\", run_type=\"chain\")\n",
    "@measure_time(\"Document Chunking\")\n",
    "def chunk_documents(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "docs = chunk_documents(data)\n",
    "print(f\"üìù Created {len(docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbeff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after chunking: 48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents after chunking: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49e7284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attentionisallyouneedgemini.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b6c80",
   "metadata": {},
   "source": [
    "## Google Generative AI Embeddings'i Kullanarak Embedding Olu≈üturma ƒ∞≈ülemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651153dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruveyda.cetin\\AppData\\Local\\miniconda3\\envs\\rag_env\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.18) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78378ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc81cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02276923693716526,\n",
       " 0.010134130716323853,\n",
       " 0.011886735446751118,\n",
       " -0.09669032692909241,\n",
       " -0.0027089761570096016]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49a28b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Embedding Creation: 0.02 seconds\n",
      "üî¢ Embedding dimension: 3072\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"create_embeddings\", run_type=\"embedding\")\n",
    "@measure_time(\"Embedding Creation\")\n",
    "def create_embeddings():\n",
    "    return GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "embeddings = create_embeddings()\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "print(f\"üî¢ Embedding dimension: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7345b",
   "metadata": {},
   "source": [
    "## ChromaDB √úzerine Kayƒ±t ƒ∞≈ülemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f73b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents=docs, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c946a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"create_vector_store\", run_type=\"chain\")\n",
    "@measure_time(\"Vector Store Creation\")\n",
    "def create_vector_store(docs, embeddings):\n",
    "    return Chroma.from_documents(\n",
    "        documents=docs, \n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "\n",
    "vector_store = create_vector_store(docs, embeddings)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":10})\n",
    "print(\"‚úÖ Vector store created and persisted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b17ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af45501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is encoder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "255c4a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b05baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 ¬∑ d) O(1) O(1)\n",
      "Recurrent O(n ¬∑ d2) O(n) O(n)\n",
      "Convolutional O(k ¬∑ n ¬∑ d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r ¬∑ n ¬∑ d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580b9c6",
   "metadata": {},
   "source": [
    "## Google Gemini API Yapƒ±sƒ±nƒ± Kullanarak LLM Tetikleme ƒ∞≈ülemleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4805eb",
   "metadata": {},
   "source": [
    "- D√º≈ü√ºk Deƒüerler (0.1-0.4): Daha kesin ve daha tutarlƒ± cevaplar verilir. Model daha tahmin edilebilir hale gelir. \n",
    "- Orta Deƒüerler(0.5-0.7): Hem mantƒ±klƒ± hem de yaratƒ±cƒ± cevaplar verilir. \n",
    "- Y√ºksek Deƒüerler (0.7-1): Daha rastgele ve yaratƒ±cƒ± , ancak bazen tutarsƒ±z yanƒ±tlar verebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b17e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.3,  # Gemini 3.0+ defaults to 1.0\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ce00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm ready for your LangSmith test. How can I help you with it?\n",
      "\n",
      "Are you looking to:\n",
      "\n",
      "*   **Test my ability to generate specific types of output?** (e.g., code, creative text, summaries)\n",
      "*   **Test my understanding of LangSmith concepts?** (e.g., tracing, evaluation, prompt management)\n",
      "*   **Test my integration with LangSmith tools?** (e.g., can I generate outputs that are easily traceable?)\n",
      "*   **Something else entirely?**\n",
      "\n",
      "Please provide me with the details of your test! I'm eager to see what you have in store.\n"
     ]
    }
   ],
   "source": [
    "# Test i√ßin basit bir LLM √ßaƒürƒ±sƒ±\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "test_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0.3)\n",
    "test_response = test_llm.invoke(\"Hello, this is a LangSmith test!\")\n",
    "print(test_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14574629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d19b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are assistant for question-answering tasks\"\n",
    "    \"Use the following pieces of context to answer the question at the end.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\" \\\n",
    "    \"Use three sentences maximum to answer.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dddf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5427ab9",
   "metadata": {},
   "source": [
    "## Soru-Cevap Zinciri Olu≈üturma ( LLM + PROMPT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56af7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eabb3",
   "metadata": {},
   "source": [
    "## RAG Zinciri Olut≈üurma ( RAG + LLM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6260173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acbdda",
   "metadata": {},
   "source": [
    "## Kullanƒ±cƒ± sorgusunu √ßalƒ±≈ütƒ±rma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28f27044",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Explain the transformer architecture?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5776c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture is a novel neural network design that relies entirely on attention mechanisms, eschewing traditional recurrent or convolutional layers. It consists of an encoder and a decoder, each composed of a stack of identical layers. Each encoder layer has a multi-head self-attention mechanism and a position-wise feed-forward network, with residual connections and layer normalization applied. The decoder includes these two sub-layers plus a third multi-head attention sub-layer that attends to the encoder's output, also incorporating residual connections and layer normalization.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
