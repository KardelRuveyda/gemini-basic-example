{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52602e2",
   "metadata": {},
   "source": [
    "## PDF Yüklemesinin Gerçekleştirlimesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"attentionisallyouneedgemini.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969a0585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a01597",
   "metadata": {},
   "source": [
    "## Veriyi parçalara ayırma(Chunking işlemi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a377a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbeff36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after chunking: 48\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents after chunking: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a49e7284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attentionisallyouneedgemini.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b6c80",
   "metadata": {},
   "source": [
    "## Google Generative AI Embeddings'i Kullanarak Embedding Oluşturma İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651153dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78378ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc81cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02276923693716526,\n",
       " 0.010134130716323853,\n",
       " 0.011886735446751118,\n",
       " -0.09669032692909241,\n",
       " -0.0027089761570096016]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7345b",
   "metadata": {},
   "source": [
    "## ChromaDB Üzerine Kayıt İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f73b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc7b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents=docs, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b17ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af45501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is encoder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "255c4a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b05baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580b9c6",
   "metadata": {},
   "source": [
    "## Google Gemini API Yapısını Kullanarak LLM Tetikleme İşlemleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4805eb",
   "metadata": {},
   "source": [
    "- Düşük Değerler (0.1-0.4): Daha kesin ve daha tutarlı cevaplar verilir. Model daha tahmin edilebilir hale gelir. \n",
    "- Orta Değerler(0.5-0.7): Hem mantıklı hem de yaratıcı cevaplar verilir. \n",
    "- Yüksek Değerler (0.7-1): Daha rastgele ve yaratıcı , ancak bazen tutarsız yanıtlar verebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b17e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.3,  # Gemini 3.0+ defaults to 1.0\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14574629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d19b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are assistant for question-answering tasks\"\n",
    "    \"Use the following pieces of context to answer the question at the end.\"\n",
    "    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\" \\\n",
    "    \"Use three sentences maximum to answer.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2dddf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5427ab9",
   "metadata": {},
   "source": [
    "## Soru-Cevap Zinciri Oluşturma ( LLM + PROMPT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d56af7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eabb3",
   "metadata": {},
   "source": [
    "## RAG Zinciri Olutşurma ( RAG + LLM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6260173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68acbdda",
   "metadata": {},
   "source": [
    "## Kullanıcı sorgusunu çalıştırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28f27044",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Explain the transformer architecture?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5776c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture is a model that relies entirely on self-attention mechanisms, eschewing recurrence and convolution. It consists of an encoder and a decoder, each composed of a stack of identical layers. Each encoder layer has a multi-head self-attention mechanism and a position-wise feed-forward network, with residual connections and layer normalization applied around each sub-layer. The decoder has these two sub-layers plus a third sub-layer for multi-head attention over the encoder's output, also incorporating residual connections and layer normalization.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
